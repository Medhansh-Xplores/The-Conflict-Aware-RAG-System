# ğŸŒŒ NebulaGears: Conflict-Aware RAG System  
### âœ¨ Google Gemini Flash + ChromaDB + Role-Based Policy Resolution

This repository implements a **Conflict-Aware Retrieval-Augmented Generation (RAG)** system designed for internal company policy support. It uses **Google Gemini Flash**, **Google text embeddings**, and **ChromaDB** to intelligently retrieve and interpret policy documents â€” even when they **conflict with each other**.

The system ensures that employees always receive the **correct policy based on their role**, not just based on keyword similarity.

---

# ğŸš€ Project Goals

Traditional RAG systems fail when documents contain *contradictory information*. This project solves that with a **conflict-aware retrieval layer** that uses:

- **Role-based prioritization**
- **Specificity scoring**
- **Policy recency**
- **LLM reasoning with structured prompts**

**Scenario Example:**  
> *User:* "I just joined as a new intern. Can I work from home?"  
Even though two documents allow remote work, the system must cite the **Intern FAQ**, which explicitly prohibits it.  
This pipeline accomplishes exactly that.

---

# ğŸ“ Repository Structure

```
nebula-rag-conflict-aware/
â”‚
â”œâ”€â”€ step1_prepare_data.py
â”œâ”€â”€ step2_build_vectorstore.py
â”œâ”€â”€ step3_retrieve.py
â”œâ”€â”€ step4_answer.py
â”‚
â”œâ”€â”€ main.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”‚
â””â”€â”€ assets/
    â””â”€â”€ intern_query_screenshot.png
```

---

# ğŸ”§ Tech Stack

| Component | Tool |
|----------|------|
| **LLM** | Google Gemini Flash (2.0 Flash or 2.5 Flash) |
| **Embeddings** | Google `text-embedding-004` |
| **Vector Database** | ChromaDB (local persistent storage) |
| **Programming Language** | Python 3 |
| **Retrieval Logic** | Cosine similarity + conflict-aware boosts |

---

# ğŸ“˜ Pipeline Overview

The system is implemented in **four clean, independent steps**.

---

## ğŸŸ¦ **Step 1 â€” Data Preparation**  
**File:** `step1_prepare_data.py`

This script processes the raw NebulaGears policy documents. It:

- Reads 3 provided documents:
  - Employee Handbook
  - Manager Updates (2024)
  - Intern Onboarding FAQ
- Cleans and normalizes text
- Splits large text into manageable RAG-friendly chunks
- Adds structured metadata:
  - `roles` (e.g., intern, manager, all_employees)
  - `policy_type`
  - `date`
  - `source filename`
- Exports everything to **prepared_docs.jsonl**

This creates a uniform dataset for embeddings and retrieval.

---

## ğŸŸ¦ **Step 2 â€” Vector Store Creation**  
**File:** `step2_build_vectorstore.py`

This script:

- Loads `prepared_docs.jsonl`
- Generates embeddings using **Google text-embedding-004**
- Creates a **local persistent ChromaDB** instance
- Inserts:
  - Embeddings
  - Document chunks
  - Metadata
  - IDs

This vector store acts as the **semantic search index** for the RAG pipeline.

---

## ğŸŸ¦ **Step 3 â€” Conflict-Aware Retrieval**  
**File:** `step3_retrieve.py`

This is the *core logic* that makes the system unique.

### ğŸ” Baseline Retrieval
- The user query is embedded with Google embeddings.
- Cosine similarity is computed against all stored vectors.

Standard retrieval would incorrectly pick Document A or B because they contain "remote work".

### âš ï¸ Why Traditional RAG Fails
Keywords like *â€œworkâ€, â€œhomeâ€, â€œremoteâ€* appear more in general policies than in the intern policy.

Therefore we apply **Conflict-Aware Boosts**:

---

## ğŸ¯ **Conflict Resolution Logic**

### ğŸ”¹ 1. **Role Boost (Most Important)**
Documents matching the userâ€™s role get a strong boost:

```
if user_role in document_roles:
    final_score *= 1.6
```

This ensures:
- Intern â†’ Intern FAQ wins  
- Manager â†’ Manager Updates win  
- General employees â†’ Handbook wins

---

### ğŸ”¹ 2. **Specificity Boost**
A document applying to fewer roles is more authoritative.

```
specificity_boost = 1 + (2 / num_roles)
```

Examples:
- Intern-only FAQ â†’ highest specificity  
- All employees â†’ lowest specificity  

---

### ğŸ”¹ 3. **Recency Boost**
More recent documents override older ones:

```
recency_boost = 1 + (year - 2023) * 0.05
```

---

### ğŸ”¹ Final Score Formula

```
final_score = cosine_similarity
              Ã— role_boost
              Ã— specificity_boost
              Ã— recency_boost
```

This guarantees the correct policy is retrieved even when others conflict.

---

## ğŸŸ¦ **Step 4 â€” Final Answer with Gemini Flash**  
**File:** `step4_answer.py`

This step uses **Google Gemini Flash** to produce the final, human-readable answer.

### The structured prompt instructs Gemini to:

- Give the **final ruling**  
- Cite exactly **one document**  
- Provide a **quote** supporting the answer  
- Explain **why this document overrides** others  
- Explain **why conflicting documents do not apply**

### Example Output (for Intern Query):

```
FINAL RULING:
Interns are required to be in the office 5 days a week...

CITATION:
intern_onboarding_faq.txt

QUOTE:
"Interns are required to be in the office 5 days a week..."

WHY THIS DOC WAS CHOSEN:
This is the only role-specific policy...

OTHER DOCUMENTS DISAGREED:
employee_handbook_v1.txt â€” general employees only
manager_updates_2024.txt â€” applies to hybrid fullâ€‘time employees
```

---

# â–¶ï¸ How to Run the Pipeline

## ğŸ”¹ Install dependencies
```
pip install -r requirements.txt
```

## ğŸ”¹ Set your Google API key
```python
import os
os.environ["GOOGLE_API_KEY"] = "YOUR_API_KEY"
```

## ğŸ”¹ Run all steps manually
```
python step1_prepare_data.py
python step2_build_vectorstore.py
python step3_retrieve.py
python step4_answer.py
```

## ğŸ”¹ OR run the entire pipeline
```
python main.py
```

---

# ğŸ“¸ Screenshot (Required)

Add your screenshot here:  
`assets/intern_query_screenshot.png`

Embed in README:

```
![Intern Query Output](assets/intern_query_screenshot.png)
```

---

# ğŸ’° Cost Analysis (Required)

### ğŸ“Œ Embedding 10,000 Documents
Assume 200 tokens/doc:

```
10,000 Ã— 200 = 2,000,000 tokens
Embedding Cost = 2M tokens / 1,000 Ã— $0.00002 = $0.04
```

### ğŸ“Œ Running 5,000 queries/day
Assume 150 tokens response:

```
5,000 Ã— 150 = 750,000 tokens/day
Monthly = 22,500,000 tokens
LLM Cost = 22.5M / 1,000 Ã— $0.00005 = $1.12 per month
```

### âœ… **Total Monthly Cost â‰ˆ $1.20**

This architecture is extremely cost-efficient and scalable.

---

# ğŸ† Final Notes

This system:

- Handles conflicting policies  
- Understands employee roles  
- Uses local vector storage  
- Produces cited, transparent answers  
- Achieves the exact behavior required by the assignment  

It ensures that even in the presence of inconsistent documents, the **correct** one is always selected and used.

---

# ğŸ‰ Done!
Your detailed README is ready.  
